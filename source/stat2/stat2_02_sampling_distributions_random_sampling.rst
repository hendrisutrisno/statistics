2. Sampling Distributions I: Random Sampling and Standard Error
============================================================

Introduction
------------

In statistical inference, we learn about a population using a sample.
We cannot trust a single sample result unless we understand how it would change under repetition.
This session introduces the idea that common summaries—such as the sample mean, sample variance, and sample proportion—are themselves random variables.
Their probability distributions are called sampling distributions.
From these sampling distributions, we define standard error, which we interpret as a “typical error” caused by sampling variation.

Learning Outcomes
-----------------

After this session, we should be able to:

- Define a random sample using the independence and identical distribution (i.i.d.) idea.
- Distinguish a parameter (population quantity) from a statistic (sample quantity).
- Treat :math:`\bar{X}`, :math:`S^2`, and :math:`\hat{p}` as random variables.
- Explain what a sampling distribution is and why it matters for inference.
- Interpret standard error as the typical size of sampling fluctuation.
- Compute standard errors for :math:`\bar{X}` and :math:`\hat{p}` in standard settings.

Connection to Previous Ideas
----------------------------

In the orientation session, we emphasized the inference workflow: define a target parameter, collect data, compute statistics, and then reason from sample to population.
In this workflow, the key conceptual shift is a “sampling mindset.”
A statistic is not a fixed number before we observe data.
It is a random variable generated by the sampling process, so it has its own distribution.

Random Sampling and the i.i.d. Model
------------------------------------

We begin with three levels of description.

A population is the full set of outcomes that could occur for a measurement process.
A sample is a subset of observed outcomes.
A random sample is a sample produced by repeating the same measurement under essentially the same conditions, with independent outcomes.

Let :math:`X` represent a measurement from the population.
A random sample of size :math:`n` is written as :math:`X_1, X_2, \ldots, X_n`.

The i.i.d. model (independent and identically distributed) states two key assumptions:

- Identically distributed: each :math:`X_i` follows the same population distribution.
- Independent: knowing one outcome does not change the distribution of another.

Under the i.i.d. model, the joint probability model factors as a product:

.. math::

   f(x_1, x_2, \ldots, x_n) \;=\; \prod_{i=1}^{n} f(x_i).

This factorization is the mathematical expression of “random sampling.”
When independence fails (for example, measurements taken in clusters or time bursts), the sampling distributions of statistics can be much wider than we expect.

Statistics as Random Variables
------------------------------

A statistic is any function of the sample values.
Because the sample changes from repetition to repetition, a statistic also changes.
Therefore, a statistic is a random variable.

We focus on three statistics that appear throughout Statistics 2.

Sample mean (center)
^^^^^^^^^^^^^^^^^^^^

.. math::

   \bar{X} \;=\; \frac{1}{n}\sum_{i=1}^{n} X_i.

Symbols:

- :math:`n` is the sample size.
- :math:`X_i` is the :math:`i`-th observation.
- :math:`\bar{X}` is the sample mean statistic.

Interpretation:

- :math:`\bar{X}` estimates :math:`\mu` (the population mean).
- The difference :math:`\bar{X} - \mu` is a sampling fluctuation, not necessarily a process change.

Sample variance (spread)
^^^^^^^^^^^^^^^^^^^^^^^^

.. math::

   S^2 \;=\; \frac{1}{n-1}\sum_{i=1}^{n}\left(X_i-\bar{X}\right)^2.

Symbols:

- :math:`S^2` is the sample variance statistic.
- The divisor :math:`n-1` adjusts for the fact that :math:`\bar{X}` is estimated from the same data.

Interpretation:

- :math:`S^2` estimates :math:`\sigma^2` (the population variance).
- :math:`S^2` often has a right-skewed sampling distribution, especially when :math:`n` is small.

Sample proportion (rate)
^^^^^^^^^^^^^^^^^^^^^^^^

For an attribute that is coded as 1 (event occurs) or 0 (event does not occur), we define:

.. math::

   \hat{p} \;=\; \frac{1}{n}\sum_{i=1}^{n} Y_i,

where :math:`Y_i \in \{0,1\}` indicates whether the event occurred on trial :math:`i`.

Symbols:

- :math:`p` is the population proportion (parameter), meaning :math:`p = P(Y=1)`.
- :math:`\hat{p}` is the sample proportion (statistic).

Interpretation:

- :math:`\hat{p}` estimates :math:`p`.
- When :math:`n` is small, :math:`\hat{p}` can take only a few discrete values (multiples of :math:`1/n`).

Sampling Distributions
----------------------

Because a statistic is a random variable, it has a probability distribution.
The sampling distribution of a statistic is the distribution induced by the sampling process.

We can describe the idea using a thought experiment:

- Fix a population model (with parameters :math:`\mu`, :math:`\sigma`, or :math:`p`).
- Repeatedly take random samples of the same size :math:`n`.
- Compute the statistic each time (such as :math:`\bar{X}`).
- The resulting collection of statistic values forms a distribution.

This distribution depends on:

- the population distribution,
- the sample size :math:`n`,
- and the sampling design (especially independence).

When an exact sampling distribution is difficult to derive, simulation (Monte Carlo approximation) is a practical alternative.
In simulation, we generate many random samples on a computer and approximate the distribution of the statistic by the empirical histogram or empirical probabilities.

Standard Error as “Typical Error”
---------------------------------

The standard error of a statistic is the standard deviation of its sampling distribution.
It measures the typical size of sampling fluctuation around the target parameter.

For the sample mean, when the population variance is :math:`\sigma^2` and the sample is i.i.d., we have:

.. math::

   E(\bar{X}) = \mu,
   \qquad
   \mathrm{Var}(\bar{X}) = \frac{\sigma^2}{n},
   \qquad
   \mathrm{SE}(\bar{X}) = \frac{\sigma}{\sqrt{n}}.

Key implications:

- If :math:`n` increases, :math:`\mathrm{SE}(\bar{X})` decreases like :math:`1/\sqrt{n}`.
- Doubling :math:`n` does not halve the standard error.
  We need :math:`4\times` the sample size to cut standard error in half.

For a Bernoulli outcome :math:`Y` with :math:`P(Y=1)=p`, the sample proportion has:

.. math::

   E(\hat{p}) = p,
   \qquad
   \mathrm{Var}(\hat{p}) = \frac{p(1-p)}{n},
   \qquad
   \mathrm{SE}(\hat{p}) = \sqrt{\frac{p(1-p)}{n}}.

In practice, :math:`\sigma` and :math:`p` are often unknown.
A common operational approach is to estimate them from the sample, and then interpret the resulting standard error as an estimated typical error.

Worked Example
--------------

Description
^^^^^^^^^^^

A filling machine dispenses detergent into bottles, as in the table below.

.. csv-table:: Example sample from the filling machine (n=36)
   :file: ../_static/data/stat2/sampling_dist_I_example_sample_table.csv
   :header-rows: 1
   :widths: 8,14,14


Let :math:`X` be the fill volume (mL) for one bottle under stable operation.

Historical engineering studies support:

- :math:`\mu = 240` mL,
- :math:`\sigma = 6` mL,
- and the process is approximately normal during stable operation.

A quality engineer takes a random sample of :math:`n = 36` bottles and computes :math:`\bar{X}`.

Question
^^^^^^^^

If the machine is truly operating at :math:`\mu = 240`, what is the probability that the sample mean is at most :math:`238.5` mL?

Analysis
^^^^^^^^

We focus on the statistic :math:`\bar{X}` because the question is about a sample average, not a single bottle.

For i.i.d. sampling from a normal population, the sampling distribution of :math:`\bar{X}` is normal with:

.. math::

   E(\bar{X}) = \mu,
   \qquad
   \mathrm{SE}(\bar{X}) = \frac{\sigma}{\sqrt{n}}.

Here:

.. math::

   \mathrm{SE}(\bar{X})
   = \frac{6}{\sqrt{36}}
   = \frac{6}{6}
   = 1 \text{ mL}.

(Note: this means a typical gap between :math:`\bar{X}` and :math:`\mu` is about 1 mL under repeated sampling.)

We standardize the event :math:`\bar{X} \le 238.5`:

.. math::

   Z
   = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}
   = \frac{238.5 - 240}{1}
   = -1.5.

Therefore:

.. math::

   P(\bar{X} \le 238.5) = P(Z \le -1.5) \approx 0.0668.

Interpretation: even when the machine is correctly centered at 240 mL, a sample mean as low as 238.5 mL can occur about 6–7% of the time due to sampling variation.

Intuition
---------

A useful mental model is that sampling creates a “distribution of summaries.”
We do not only have a distribution for individual measurements :math:`X`.
We also have a distribution for :math:`\bar{X}`, and this distribution is tighter.

Another useful mental model is to treat standard error as an operational yardstick.
If the observed statistic is only one or two standard errors away from the parameter value suggested by historical baselines, the discrepancy may be typical sampling noise.
If it is many standard errors away, the discrepancy is less compatible with the baseline model and may justify deeper investigation.

Figure 1 — One sample and its statistics
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In this figure, the data represent one random sample of :math:`n=30` fill volumes from a stable filling machine.
We hold the baseline process mean :math:`\mu` fixed, but the observed points change because sampling is random.
The markers are the individual observations in the order they were collected (each dot is one bottle).
The horizontal line labeled :math:`\bar{x}` is the sample mean, and the shaded band shows :math:`\bar{x} \pm s`, using the sample standard deviation as a within-sample spread measure.
The key interpretation is that :math:`\bar{x}` is not guaranteed to equal :math:`\mu`; the gap is a sampling fluctuation, which motivates studying the sampling distribution of :math:`\bar{X}`.

.. raw:: html

   <iframe src="../_static/figures/stat2/02_01_sampling_dist_I_fig1_random_sample.html" scrolling="no" style="width:95%; height:520px; border:none; overflow:hidden; display:block; margin:0 auto;"> </iframe>

Figure 2 — Sampling distribution of the sample mean and standard error
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In this figure, the process context is the same filling operation, but now we repeat the sampling experiment many times on a computer.
We hold the population model fixed at :math:`\mu=240` mL and :math:`\sigma=6` mL, and we change only the sample size :math:`n`.
The light-gray histogram (trace: “Individual observations :math:`X`”) shows the variability of single-bottle fill volumes.
The colored histogram (trace: “Sampling distribution of :math:`\bar{X}`”) shows the distribution of the sample mean from repeated samples of size :math:`n`.
The black curve (trace: “Reference Normal for :math:`\bar{X}`”) is the model-based sampling distribution with standard error :math:`\sigma/\sqrt{n}`.
Because the x-axis range is fixed, we can directly compare spreads as :math:`n` increases, and we observe that the sampling distribution becomes tighter, which operationally means the typical error of :math:`\bar{X}` decreases.

.. raw:: html

   <iframe src="../_static/figures/stat2/02_02_sampling_dist_I_fig2_sampling_dist_mean.html" scrolling="no" style="width:95%; height:520px; border:none; overflow:hidden; display:block; margin:0 auto;"> </iframe>

Figure 3 — Sampling distribution of the sample proportion for an underfill event
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In this figure, we convert the same quality problem into an event-rate problem.
Each bottle is labeled :math:`Y=1` if it is underfilled (below a specification threshold) and :math:`Y=0` otherwise, so :math:`p=P(Y=1)` is the underfill rate.
The light-gray bar trace (trace: “Individual observations :math:`Y`”) shows the population probabilities at 0 and 1.
The colored bar trace (trace: “Sampling distribution of :math:`\hat{p}`”) shows how the observed sample proportion varies across repeated samples of size :math:`n`.
The black curve (trace: “Reference Normal for :math:`\hat{p}`”) is a model-based approximation centered at :math:`p` with standard error :math:`\sqrt{p(1-p)/n}`.
The comparison is meaningful because the x-axis is fixed from 0 to 1, so tightening of the :math:`\hat{p}` distribution reflects a real reduction in typical sampling error as :math:`n` increases.

.. raw:: html

   <iframe src="../_static/figures/stat2/02_03_sampling_dist_I_fig3_sampling_dist_phat.html" scrolling="no" style="width:95%; height:520px; border:none; overflow:hidden; display:block; margin:0 auto;"> </iframe>

Discussion and Common Errors
----------------------------

1) Confusing parameters with statistics.
A parameter (such as :math:`\mu` or :math:`p`) is a population quantity and is fixed in the model.
A statistic (such as :math:`\bar{X}` or :math:`\hat{p}`) is random before sampling.
To avoid the error, we explicitly state whether a symbol refers to the population model or to the realized sample.

2) Treating standard deviation and standard error as the same concept.
The standard deviation :math:`\sigma` describes variability of individual observations.
The standard error describes variability of a statistic across repeated samples.
To avoid the error, we always attach the name of the object: “spread of :math:`X`” versus “spread of :math:`\bar{X}`.”

3) Using a convenience sample but reasoning as if it were random.
Nonrandom selection can create systematic bias, even if :math:`n` is large.
The resulting sampling distribution may be centered away from the parameter.
To avoid the error, we check the data collection mechanism and ensure independence and representativeness are plausible.

Summary
-------

- Random sampling is modeled by i.i.d. observations :math:`X_1,\ldots,X_n` from a population distribution.
- A statistic is a function of the sample, so it is a random variable before we observe data.
- The sampling distribution of a statistic describes how that statistic behaves under repeated sampling.
- Standard error is the standard deviation of a statistic’s sampling distribution and acts as a typical sampling error.
- For i.i.d. sampling, :math:`\mathrm{SE}(\bar{X})=\sigma/\sqrt{n}` and :math:`\mathrm{SE}(\hat{p})=\sqrt{p(1-p)/n}` in standard settings.
- Source alignment: definitions and motivation follow standard treatments of random sampling and sampling distributions. :contentReference[oaicite:0]{index=0} :contentReference[oaicite:1]{index=1}
